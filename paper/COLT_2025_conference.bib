@article{zhang2022near,
  title={Near-optimality for infinite-horizon restless bandits with many arms},
  author={Zhang, Xiangyu and Frazier, Peter I},
  journal={arXiv preprint arXiv:2203.15853},
  year={2022}
}
@article{zhang2021restless,
  title={Restless bandits with many arms: Beating the central limit theorem},
  author={Zhang, Xiangyu and Frazier, Peter I},
  journal={arXiv preprint arXiv:2107.11911},
  year={2021}
}
@article{brown2020index,
  title={Index policies and performance bounds for dynamic selection problems},
  author={Brown, David B and Smith, James E},
  journal={Management Science},
  volume={66},
  number={7},
  pages={3029--3050},
  year={2020},
  publisher={INFORMS}
}
@article{zayas2019asymptotically,
  title={An asymptotically optimal heuristic for general nonstationary finite-horizon restless multi-armed, multi-action bandits},
  author={Zayas-Caban, Gabriel and Jasin, Stefanus and Wang, Guihua},
  journal={Advances in Applied Probability},
  volume={51},
  number={3},
  pages={745--772},
  year={2019}
}
@article{hu2017asymptotically,
  title={An asymptotically optimal index policy for finite-horizon restless bandits},
  author={Hu, Weici and Frazier, Peter},
  journal={arXiv preprint arXiv:1707.00205},
  year={2017}
}
@article{ioannidis2016adaptive,
  title={Adaptive caching networks with optimality guarantees},
  author={Ioannidis, Stratis and Yeh, Edmund},
  journal={ACM SIGMETRICS Performance Evaluation Review},
  volume={44},
  number={1},
  pages={113--124},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{gast2024reoptimization,
      title={Reoptimization Nearly Solves Weakly Coupled Markov Decision Processes}, 
      author={Nicolas Gast and Bruno Gaujal and Chen Yan},
      journal={arxiv preprint arxiv:2211.01961},
      year={2024}
}
      %eprint={2211.01961},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2211.01961}, 
}

@article{verloop2016asymptotically,
  title={ASYMPTOTICALLY OPTIMAL PRIORITY POLICIES FOR INDEXABLE AND NONINDEXABLE RESTLESS BANDITS},
  author={Verloop, IM},
  journal={THE ANNALS of APPLIED PROBABILITY},
  pages={1947--1995},
  year={2016},
  publisher={JSTOR}
}
@article{ghosh2022indexability,
  title={Indexability is not enough for whittle: Improved, near-optimal algorithms for restless bandits},
  author={Ghosh, Abheek and Nagaraj, Dheeraj and Jain, Manish and Tambe, Milind},
  journal={arXiv preprint arXiv:2211.00112},
  year={2022}
}

@phdthesis{chen:tel-04068056,
  TITLE = {{Close-to-opimal policies for Markovian bandits}},
  AUTHOR = {Yan, Chen},
  NUMBER = {2022GRALM046},
  SCHOOL = {{Universit{\'e} Grenoble Alpes [2020-2023]}},
  YEAR = {2022},
  MONTH = Dec,
  KEYWORDS = {Probability ; Markovian Bandit ; Stochastic optimization ; Probabilit{\'e}s ; Bandit Markovian ; Optimisation stochastique},
  TYPE = {Theses},
  PDF = {https://theses.hal.science/tel-04068056v2/file/YAN_2022_archivage.pdf},
  HAL_ID = {tel-04068056},
  HAL_VERSION = {v2},
}

@article{PT99,
 ISSN = {0364765X, 15265471},
 abstract = {We show that several well-known optimization problems related to the optimal control of queues are provably intractable-independently of any unproven conjecture such as P ≠ NP. In particular, we show that several versions of the problem of optimally controlling a simple network of queues with simple arrival and service distributions and multiple customer classes is complete for exponential time. This is perhaps the first such intractability result for a well-known optimization problem. We also show that the restless bandit problem (the generalization of the multi-armed bandit problem to the case in which the unselected processes are not quiescent) is complete for polynomial space.},
 author = {Christos H. Papadimitriou and John N. Tsitsiklis},
 journal = {Mathematics of Operations Research},
 number = {2},
 pages = {293--305},
 publisher = {INFORMS},
 title = {The Complexity of Optimal Queuing Network Control},
 urldate = {2024-09-16},
 volume = {24},
 year = {1999}
}

@article{Wh88,
 ISSN = {00219002},
 abstract = {We consider a population of n projects which in general continue to evolve whether in operation or not (although by different rules). It is desired to choose the projects in operation at each instant of time so as to maximise the expected rate of reward, under a constraint upon the expected number of projects in operation. The Lagrange multiplier associated with this constraint defines an index which reduces to the Gittins index when projects not being operated are static. If one is constrained to operate m projects exactly then arguments are advanced to support the conjecture that, for m and n large in constant ratio, the policy of operating the m projects of largest current index is nearly optimal. The index is evaluated for some particular projects.},
 author = {P. Whittle},
 journal = {Journal of Applied Probability},
 pages = {287--298},
 publisher = {Applied Probability Trust},
 title = {Restless Bandits: Activity Allocation in a Changing World},
 urldate = {2024-09-16},
 volume = {25},
 year = {1988}
}

@article{HXCW24,
    author = {Yige Hong and Qiaomin Xie and Yudong Chen and Weina Wang},
    title = {When is exponential asymptotic optimality achievable in average-reward restless bandits?},
    journal = {arxiv preprint arxiv:2405.17882},
    year = {2024}
}

@inproceedings{HXCW23,
title={Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption},
author={Yige Hong and Qiaomin Xie and Yudong Chen and Weina Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@Article{GGY23,
  author={Nicolas Gast and Bruno Gaujal and Chen Yan},
  title={{Exponential asymptotic optimality of Whittle index policy}},
  journal={Queueing Systems: Theory and Applications},
  year=2023,
  volume={104},
  number={1},
  pages={107-150},
  month={June},
  keywords={Multi-armed bandits; Whittle index; Asymptotic optimality},
  doi={10.1007/s11134-023-09875-},
  abstract={ We evaluate the performance of Whittle index policy for restless Markovian bandit. It is shown in Weber and Weiss (J Appl Probab 27(3):637–648, 1990) that if the bandit is indexable and the associated deterministic system has a global attractor fixed point, then the Whittle index policy is asymptotically optimal in the regime where the arm population grows proportionally with the number of activation arms. In this paper, we show that, under the same conditions, this convergence rate is exponential in the arm population, unless the fixed point is singular (to be defined later), which almost never happens in practice. Our result holds for the continuous-time model of Weber and Weiss (1990) and for a discrete-time model in which all bandits make synchronous transitions. Our proof is based on the nature of the deterministic equation governing the stochastic system: We show that it is a piecewise affine continuous dynamical system inside the simplex of the empirical measure of the arms. Using simulations and numerical solvers, we also investigate the singular cases, as well as how the level of singularity influences the (exponential) convergence rate. We illustrate our theorem on a Markovian fading channel model.}
}

@article{GGY23b,
author = {Gast, Nicolas and Gaujal, Bruno and Yan, Chen},
title = {Linear Program-Based Policies for Restless Bandits: Necessary and Sufficient Conditions for (Exponentially Fast) Asymptotic Optimality},
journal = {Mathematics of Operations Research},
volume = {},
number = {},
pages = {},
year = {2023},
doi = {10.1287/moor.2022.0101},
eprint = {    
        https://doi.org/10.1287/moor.2022.0101
}
,
    abstract = { We provide a framework to analyze control policies for the restless Markovian bandit model under both finite and infinite time horizons. We show that when the population of arms goes to infinity, the value of the optimal control policy converges to the solution of a linear program (LP). We provide necessary and sufficient conditions for a generic control policy to be (i) asymptotically optimal, (ii) asymptotically optimal with square root convergence rate, and (iii) asymptotically optimal with exponential rate. We then construct the LP-index policy that is asymptotically optimal with square root convergence rate on all models and with exponential rate if the model is nondegenerate in finite horizon and satisfies a uniform global attractor property in infinite horizon. We next define the LP-update policy, which is essentially a repeated LP-index policy that solves a new LP at each decision epoch. We conclude by providing numerical experiments to compare the efficiency of different LP-based policies.Funding: This work was supported by Agence Nationale de la Recherche [Grant ANR-19-CE23-0015]. }
}

@INPROCEEDINGS{yan2024,
  author={Yan, Chen},
  booktitle={2024 IEEE 63rd Conference on Decision and Control (CDC)}, 
  title={An Optimal-Control Approach to Infinite-Horizon Restless Bandits: Achieving Asymptotic Optimality with Minimal Assumptions}, 
  year={2024},
  volume={},
  number={},
  pages={6665-6672},
  keywords={Sufficient conditions;Markov decision processes;Stationary state;Focusing;Trajectory;Complexity theory;Time factors;Convergence},
  doi={10.1109/CDC56724.2024.10886365}}

@article{WW90,
 ISSN = {00219002},
 abstract = {We investigate the optimal allocation of effort to a collection of n projects. The projects are 'restless' in that the state of a project evolves in time, whether or not it is allocated effort. The evolution of the state of each project follows a Markov rule, but transitions and rewards depend on whether or not the project receives effort. The objective is to maximize the expected time-average reward under a constraint that exactly m of the n projects receive effort at any one time. We show that as m and n tend to ∞ with m/n fixed, the per-project reward of the optimal policy is asymptotically the same as that achieved by a policy which operates under the relaxed constraint that an average of m projects be active. The relaxed constraint was considered by Whittle (1988) who described how to use a Lagrangian multiplier approach to assign indices to the projects. He conjectured that the policy of allocating effort to the m projects of greatest index is asymptotically optimal as m and n tend to ∞. We show that the conjecture is true if the differential equation describing the fluid approximation to the index policy has a globally stable equilibrium point. This need not be the case, and we present an example for which the index policy is not asymptotically optimal. However, numerical work suggests that such counterexamples are extremely rare and that the size of the suboptimality which one might expect is minuscule.},
 author = {Richard R. Weber and Gideon Weiss},
 journal = {Journal of Applied Probability},
 number = {3},
 pages = {637--648},
 publisher = {Applied Probability Trust},
 title = {On an Index Policy for Restless Bandits},
 urldate = {2024-09-16},
 volume = {27},
 year = {1990}
}

@inproceedings{AGND23,
author = {Ghosh, Abheek and Nagaraj, Dheeraj and Jain, Manish and Tambe, Milind},
title = {Indexability is Not Enough for Whittle: Improved, Near-Optimal Algorithms for Restless Bandits},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the problem of planning restless multi-armed bandits (RMABs) with multiple actions. This is a popular model for multi-agent systems with applications like multi-channel communication, monitoring and machine maintenance tasks, and healthcare. Whittle index policies, which are based on Lagrangian relaxations, are widely used in these settings due to their simplicity and near-optimality under certain conditions. In this work, we first show that Whittle index policies can fail in simple and practically relevant RMAB settings, even when the RMABs are indexable. We discuss why the optimality guarantees fail and why asymptotic optimality may not translate well to practically relevant planning horizons.We then propose an alternate planning algorithm based on the mean-field method, which can provably and efficiently obtain near-optimal policies with a large number of arms, without the stringent structural assumptions required by the Whittle index policies. This borrows ideas from existing research with some improvements: our approach is hyper-parameter free, and we provide an improved non-asymptotic analysis which has: (a) no requirement for exogenous hyper-parameters and tighter polynomial dependence on known problem parameters; (b) high probability bounds which show that the reward of the policy is reliable; and (c) matching sub-optimality lower bounds for this algorithm with respect to the number of arms, thus demonstrating the tightness of our bounds. Our extensive experimental analysis shows that the mean-field approach matches or outperforms other baselines},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1294–1302},
numpages = {9},
keywords = {mean-field, resource allocation, restless bandits, whittle index},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{DTGLS14,
author = {Damm, Tobias and Gr\"{u}ne, Lars and Stieler, Marleen and Worthmann, Karl},
title = {An Exponential Turnpike Theorem for Dissipative Discrete Time Optimal Control Problems},
journal = {SIAM Journal on Control and Optimization},
volume = {52},
number = {3},
pages = {1935-1957},
year = {2014},
doi = {10.1137/120888934},
eprint = {    
        https://doi.org/10.1137/120888934
}
,
    abstract = { We investigate the exponential turnpike property for finite horizon undiscounted discrete time optimal control problems without any terminal constraints. Considering a class of strictly dissipative systems, we derive a boundedness condition for an auxiliary optimal value function which implies the exponential turnpike property. Two theorems illustrate how this boundedness condition can be concluded from structural properties like controllability and stabilizability of the control system under consideration. }
}

@article{Avrachenkov2020WhittleIB,
  title={Whittle index based Q-learning for restless bandits with average reward},
  author={Konstantin Avrachenkov and Vivek S. Borkar},
  journal={preprint arXiv:2004.14427},
  year={2020}
}

@InProceedings{Avrachenkov2021,
author="Avrachenkov, Konstantin E.
and Borkar, Vivek S.
and Dolhare, Hars P.
and Patil, Kishor",
editor="Piunovskiy, Alexey
and Zhang, Yi",
title="Full Gradient DQN Reinforcement Learning: A Provably Convergent Scheme",
booktitle="Modern Trends in Controlled Stochastic Processes:",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="192--220",
abstract="We analyze the DQN reinforcement learning algorithm as a stochastic approximation scheme using the o.d.e. (for `ordinary differential equation') approach and point out certain theoretical issues. We then propose a modified scheme called Full Gradient DQN (FG-DQN, for short) that has a sound theoretical basis and compare it with the original scheme on sample problems. We observe a better performance for FG-DQN.",
isbn="978-3-030-76928-4"
}

%{Avrachenkov2024,
      %title={Asymptotically Optimal Policies for Weakly Coupled Markov Decision Processes}, 
      author={Diego Goldsztajn and Konstantin Avrachenkov},
      year={2024},
      eprint={2406.04751},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2406.04751}, 
}

@article{Avrachenkov2024,
      title={Asymptotically Optimal Policies for Weakly Coupled Markov Decision Processes}, 
      author={Diego Goldsztajn and Konstantin Avrachenkov},
      journal={arXiv preprint arXiv:2406.04751},
      year={2024}
}
      %volume={2406.04751},
      archivePrefix={arXiv},
      primaryClass={math.OC} 
}

@book{puterman2014markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, M.L.},
  isbn={9781118625873},
  series={Wiley Series in Probability and Statistics},
  year={2014},
  publisher={Wiley}
}

@article{veatch1996scheduling,
  title={Scheduling a make-to-stock queue: Index policies and hedging points},
  author={Veatch, Michael H and Wein, Lawrence M},
  journal={Operations Research},
  volume={44},
  number={4},
  pages={634--647},
  year={1996},
  publisher={INFORMS}
}

@article{nino2002dynamic,
  title={Dynamic allocation indices for restless projects and queueing admission control: a polyhedral approach},
  author={Nino-Mora, Jos{\'e}},
  journal={Mathematical programming},
  volume={93},
  number={3},
  pages={361--413},
  year={2002},
  publisher={Springer}
}

@article{dance2019optimal,
  title={Optimal policies for observing time series and related restless bandit problems},
  author={Dance, Christopher R and Silander, Tomi},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={35},
  pages={1--93},
  year={2019}
}

@article{MearaWebcrawl01,
  title={A topic-specific Web robot model based on restless bandits},
  author={O'Meara, Tadhg and Patel, Ahmed},
  journal={IEEE Internet Computing},
  volume={5},
  number={2},
  pages={27--35},
  year={2001},
  publisher={IEEE}
}

@Article{NinoMora23,
AUTHOR = {Niño-Mora, José},
TITLE = {Markovian Restless Bandits and Index Policies: A Review},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {7},
ARTICLE-NUMBER = {1639},
ISSN = {2227-7390},
ABSTRACT = {The restless multi-armed bandit problem is a paradigmatic modeling framework for optimal dynamic priority allocation in stochastic models of wide-ranging applications that has been widely investigated and applied since its inception in a seminal paper by Whittle in the late 1980s. The problem has generated a vast and fast-growing literature from which a significant sample is thematically organized and reviewed in this paper. While the main focus is on priority-index policies due to their intuitive appeal, tractability, asymptotic optimality properties, and often strong empirical performance, other lines of work are also reviewed. Theoretical and algorithmic developments are discussed, along with diverse applications. The main goals are to highlight the remarkable breadth of work that has been carried out on the topic and to stimulate further research in the field.},
DOI = {10.3390/math11071639}
}

@article{hong2024unichain,
    author={Yige Hong and Qiaomin Xie and Yudong Chen and Weina Wang},
    title={Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits},
    journal = {arXiv preprint arXiv:2402.05689},
    year = {2024}
}

@book{hernandez2012discrete,
  title={Discrete-Time Markov Control Processes: Basic Optimality Criteria},
  author={Hernandez-Lerma, O. and Lasserre, J.B.},
  isbn={9781461207290},
  lccn={95037683},
  series={Stochastic Modelling and Applied Probability},
  year={2012},
  publisher={Springer New York}
}

@article{Thompson33,
    author = {Thompson, William R},
    title = "{On The Likelihood That One Unknown Probability Exceeds Another In View Of The Evidence Of Two Samples}",
    journal = {Biometrika},
    volume = {25},
    number = {3-4},
    pages = {285-294},
    year = {1933},
    month = {12},
    issn = {0006-3444},
    doi = {10.1093/biomet/25.3-4.285},
    eprint = {https://academic.oup.com/biomet/article-pdf/25/3-4/285/513725/25-3-4-285.pdf},
}

@Book{Wald:1947,
  author =       "Wald, Abraham",
  title =        "Sequential Analysis",
  publisher =    "John Wiley and Sons",
  year =         "1947",
  edition =   "1st",
  bib2html_rescat = "Bandits",
}

@book{bellman1957dynamic,
  title={Dynamic Programming},
  author={Bellman, R. and Bellman, R.E. and Rand Corporation},
  lccn={lc57005444},
  series={Rand Corporation research study},
  year={1957},
  publisher={Princeton University Press}
}

@article{Gittin79,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2985029},
 abstract = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
 author = {J. C. Gittins},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {148--177},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Bandit Processes and Dynamic Allocation Indices},
 urldate = {2024-09-25},
 volume = {41},
 year = {1979}
}


@article{Glazebrook15,
 ISSN = {00018678},
 abstract = {The class of restless bandits as proposed by Whittle (1988) have long been known to be intractable. This paper presents an optimality result which extends that of Weber and Weiss (1990) for restless bandits to a more general setting in which individual bandits have multiple levels of activation but are subject to an overall resource constraint. The contribution is motivated by the recent works of Glazebrook et al. (2011a), (2011b) who discussed the performance of index heuristics for resource allocation in such systems. Hitherto, index heuristics have been shown, under a condition of full indexability, to be optimal for a natural Lagrangian relaxation of such problems in which a resource is purchased rather than constrained. We find that under key assumptions about the nature of solutions to a deterministic differential equation that the index heuristics above are asymptotically optimal in a sense described by Whittle. We then demonstrate that these assumptions always hold for three-state bandits.},
 author = {D. J. Hodge and K. D. Glazebrook},
 journal = {Advances in Applied Probability},
 number = {3},
 pages = {652--667},
 publisher = {Applied Probability Trust},
 title = {On The Asymptotic Optimality Of Greedy Index Heuristics For Multi-Action Restless Bandits},
 urldate = {2024-09-25},
 volume = {47},
 year = {2015}
}

@article{yan2024optimalgap,
    author = {Chen Yan and Weina Wang and Lei Ying},
    title = {Achieving O(1/N) Optimality Gap in Restless Bandits through Diffusion Approximation},
    journal = {arXiv preprint arXiv:2410.15003},
    year = {2024}
}

@book{altman1999constrained,
  title={Constrained Markov Decision Processes},
  author={Altman, E.},
  isbn={9780849303821},
  lccn={99210415},
  series={Stochastic Modeling Series},
  url={https://books.google.fr/books?id=3X9S1NM2iOgC},
  year={1999},
  publisher={Taylor \& Francis}
}