\section{System Model and linear relaxation}
\label{sec:model}
\subsection{System Model}
We consider an infinite horizon discrete time restless Markovian bandit problem parameterized by the tuple $\langle \sspace, \Pmat{0}, \Pmat{1}, \Rvec{0}, \Rvec{1}; \alpha, N \rangle$.  A decision maker is facing $N$ statistically identical arms, and each arm has a state that belongs to the finite state-space $\sspace$. At each time-instant, the decision maker observes the states of all arms $\bs=\{s_1\dots s_N\}$ and chooses a vector of actions $\ba=\{a_1\dots a_N\}$, where the action $a_n=1$ (respectively $a_n=0$) corresponds to pulling the arm $n$ (or leaving it). The decision maker is constrained to pull at most $\alpha N$ arms at each decision epoch. 

The matrices $\Pmat{0}$ and $\Pmat{1}$ denote the transitions matrices of each arm and the vectors $\Rvec{0}, \Rvec{1}$ denote the $|\sspace|$ dimensional vector for the rewards. We assume that all the rewards $\Rvec{}$ lie between $0$ and $1$. As the state-space is finite, this assumption can be made without loss of generality by scaling and centering the reward vector. 
We assume that the transitions of all arms are Markovian and independent. This means that if the arms are in state $\bs := \{s_1, s_2 \dots s_N\}\in\sspace^N$ and the decision maker takes an action $\ba\in\{0,1\}^N$, then the decision maker earns a reward $\frac{1}{N}\sum_{n} r^{a_n}_{s_n}$ and the next state becomes $\bs':= \{s'_1, s'_2 \dots s'_N\}\in\sspace^N$ with probability
\begin{equation}\label{EQ:MKEVOL}
    \begin{split}
            &\ProbP(\bS(t + 1) = \bs'|\bS(t) = \bs,\bA(t) = \ba, \dots \bS(0), \bA(0)) \\
            &\qquad= \ProbP(\bS(t + 1) = \bs'|\bS(t) = \bs,\bA(t) = \ba)
            = \Pi_{n = 1}^{N} P^{a_n}_{s_n, s_n'}.
    \end{split}
\end{equation}
%Where $P^{a_n}_{s_n', s_n}$ denotes the $(s'_n, s_n)$ entry of the corresponding transition matrix $\Pmat{a_n}$. 
It is important to note that the arms are only coupled through the budget constraint $\alpha$.

Let $\pi$ be a stationary policy mapping each state to a probability of choosing actions, i.e, $\pi : \sspace^{N} \to \Delta (\{0, 1\}^{N})$ subject to the budget constraint $\alpha$. Let $\pspace$ denote the space of all such policies. We define the average gain of policy $\pi\in\pspace$ as
\begin{equation}\label{EQ::VPOLT}
    \Vpol{N} = \lim_{\THor \to \infty}\frac{1}{\THor}\E_{\pi}\left[ \sum_{t = 0}^{\THor - 1} \frac{1}{N} \sum_{n = 1}^N r^{A_n(t)}_{S_n(t)}\right].
\end{equation}
In theory, the average gain of a policy might depend on the initial state $\bS^{(N)}(0)$. Yet, under mild conditions (which will be verified in our case), this value does not depend on the initial state. This is why we omit the dependence on the initial state and simply write $\Vpol{N}$.
 
Here $(S_n(t), A_n(t))$ denotes the state-action pair of the $n$th arm at time $t$ and $r^{A_n(t)}_{S_n(t)}$ denotes the $S_n(t)^{\text{th}}$ entry of the $\Rvec{A_n(t)}$ vector. As the state-space and action space is finite, for any stationary policy $\pi$, the limit is well defined, \cite{puterman2014markov}. The RMAB problem amounts to computing a policy $\pi$ that maximizes the infinite average reward. We denote the optimal value of the problem as
\begin{align}\label{EQ::VOPTT}
    \Vopt{N} &:= \max_{\pi \in \pspace} \Vpol{N}.%  \nonumber\\
    %\text{s.t.} \hspace{0.25 in} &\sum_{s} \YN{t}{s}{1} = \alpha \label{EQ:VOPT2}\\
%    \text{follows}&\text{ the Markovian evolution \eqref{EQ:MKEVOL}}
\end{align}
%As the states and actions are finite, such an optimal policy exists that achieves . 
The optimal policy exists and the limit is well defined, \citet{puterman2014markov}. 

\subsection{Alternative state representation via empirical distribution}

In order to build an approximation of \eqref{EQ::VOPTT}, we introduce an alternative representation of the state space, that we will use extensively in the paper. Given any joint state of the arms $\bs\in\sspace^{N}$, we denote the empirical distribution of these arms as $\mB(\bs)\in\Delta_S$, where $\Dels$ is the simplex of dimension $|\sspace|$. $\mB(\bs)$ is a vector with $|\sspace|$ dimensions and $x_i(\bs)$ is the fraction of arms that are in state $i$.  Next, given an action vector $\ba$, we denote by $\bu(\bs, \ba)$ the empirical distribution of the state-action pairs ($s,1$). In words, $u_i(\bs,\ba)$ is the fraction of arms that are in state $i$ and that are pulled.  Since no more than $N\alpha$ arms can be pulled at any time instance and no more than $Nx_i(\bs)$ arms can be pulled in state $i$, it follows that when $\mB$ is fixed, $\bu$ satisfies the following inequalities,
    \begin{equation}\label{EQ::POL}
    0 \leq \bu \leq \mB \hspace{0.3 in} \|\bu\|_1 \leq \alpha \|\mB\|_1 = \alpha,
\end{equation}
where $\bu\leq\mB$ denotes a component-wise inequality and $\|\cdot \|_1$ denotes the $l_1$ norm. We denote by $\mcl{U}(\mB)$ the set of feasible actions for a given $\mB$, \emph{i.e.}, the set of $\bu$ that satisfy \eqref{EQ::POL}.

\subsection{Linear relaxation}

We consider the following linear program:
\begin{subequations}
    \label{EQ::VOPTDYN-Tinf}
    \begin{align}
       \Vopt{\infty} := \max_{\bx,\bu\in\Delta_{\sspace}}&~ \Rvec{0}\cdot \mB + (\Rvec{1} - \Rvec{0})\cdot\bu,\label{EQ:VOPTDYN-inf1}\\
        \text{Subject to: }\quad
        &\bu \in \mcl{U}(\mB) \label{EQ::MF3-inf}\\
        &\mB = \mB  \Pmat{0} +  \bu (\Pmat{1} - \Pmat{0}) \label{EQ::MF2-inf}
    %\max_{\pi \in \pspacedyn}\Vpol{\infty}{\mB}
    \end{align}
\end{subequations}
This linear program is known to be a relaxation of \eqref{EQ::VOPTT} that is asymptotically tight, that is $\Vopt{N}\le \Vopt{\infty}$ for all $N$ and $\lim_{N\to\infty}\Vopt{N}=\Vopt{\infty}$, see \citet{GGY23b,HXCW24}.%note this comes from theorem 2.1 in Chen's paper, the idea is to look at gain at the fixed point which allows for dealing with a policy without worrying about constraint violations

To give some intuition on the relationship between \eqref{EQ::VOPTT} and \eqref{EQ::VOPTDYN-Tinf}, we remark that if $\bX{t} := \mB(\bS^{N}(t))$ is the empirical distribution of states at time $t$ and $\bU(t)=\bu(\bS(t),\bA(t))$ is the joint control, then it is shown in \citet{GGY23b} that the Markovian evolution \eqref{EQ:MKEVOL} implies
\begin{equation}
\E[\bX{t + 1} \mid \bX{t}, \bU(t)] = \mX{(t)}  \Pmat{0} +  \bU(t)(\Pmat{1} - \Pmat{0}).
\label{EQ:evolution_mf}
% + \mM(\mX{(t)}, \bU(t))
\end{equation}
In \eqref{EQ::VOPTDYN-Tinf}, the variable $x_i$ corresponds to the time-averaged fraction of arms in state $i$; similarly the variable $u_i$ corresponds to the time-averaged fraction of arms in state $i$ that are pulled. The constraint \eqref{EQ::MF3-inf} imposes that \emph{on average}, no more than $\alpha N$ arms are pulled. This is in contrast with the condition imposed for problem \eqref{EQ::VOPTT} that enforces this condition at each time step. 





\section{Construction of the LP-update policy}
\label{sec:algo}

\subsection{The Finite-Horizon Mean Field Control Problem}
%As stated in the introduction, this problem is P-SPACE hard, we will instead look for asymptotically optimal algorithms that converge to the optimal algorithm as $N$ tends to infinity. 
%We have a finite set of states $\sspace \triangleq \{1, 2 \dots d\}$. 
% Given any joint state of the arms $\bs\in\sspace^{N}$, we denotes the empirical distribution of these arms as $\mB(\bs)\in\Delta_S$, where $\Dels$ is the simplex of dimension $|\sspace|$. $\mB(\bs)$ is a vector with $|\sspace|$ dimensions and $x_i(\bs)$ is the fraction of arms that are in state $i$.  Next, given an action vector $\ba$, we denote by $\bu(\bs, \ba)$ the empirical distribution of the state-action pairs ($s,1$). In words, $u_i(\bs,\ba)$ is the fraction of arms that are in state $i$ and that are pulled.  Since no more than $N\alpha$ arms can be pulled at any time instance and no more than $N\mB_{s}$ arms can be pulled in state $s$, it follows that when $\mB$ is fixed, $\bu$ satisfies the following inequalities,
%     \begin{equation}\label{EQ::POL}
%     \bu \leq \mB \hspace{0.3 in} \|\bu\|_1 \leq \alpha \|\mB\|_1 = \alpha 
% \end{equation}
%  Here $\leq$ denotes a component-wise inequality and $\|\cdot \|_1$ denotes the $l_1$ norm. We denote by $\mcl{U}(\mB)$ the set of feasible actions for a given $\bx$, \emph{i.e.}, the set of $\bu$ that satisfy \eqref{EQ::POL}.

%  If $\bX{t} := \mB(\bS^{N}(t))$ denotes the empirical distribution of the states at time $t$ and $\bU(t):\bu(\bS(t),\bA(t))$ denote the joint control, then it is is shown in \citet{GGY23b} that the Markovian evolution \eqref{EQ:MKEVOL} implies
%  \begin{equation}
%      \E[\bX{t + 1} \mid \bX{t}, \bA(t)] = \Pmat{0} \mX{(t)} + (\Pmat{1} - \Pmat{0}) \bU(t).% + \mM(\mX{(t)}, \bU(t))
%  \end{equation}
 
% where $\mM(\cdot, \cdot)$ is a Markovian random vector with the following properties,
% \[
% \E[\mM(\mX{}, \bU)|\mX{}, \bU] = 0, \hspace{0.3 in} \E[\|\mM(\mX{}, \bU)\|_1|\mX{},\bU]\leq \frac{|\sspace|}{\sqrt{N}}
% \]


To build the LP-update policy, we consider a \emph{controlled dynamical system}, also called the \emph{mean field model}, that is a finite-time equivalent of \eqref{EQ::VOPTDYN-Tinf}. For a given initial condition $\bx(0)$ and a time-horizon $\tau$, the states and actions of this dynamical system are constrained by the evolution equations 
\begin{subequations}
    \label{EQ:MeanField}
    \begin{align}
        %\mB(0) &= \mB(\bS^{N}(0)) \label{EQ::MF1}\\
        \bu(t) &\in \mcl{U}(\bx(t))  \label{EQ::MF3} \\
        \bx({t + 1}) &= \bx(t) \Pmat{0}  + \bu(t) (\Pmat{1} - \Pmat{0}) , \label{EQ::MF2}
    \end{align}
\end{subequations}
$\forall t\in\{0\dots \tau-1\}$. In the above equation, \eqref{EQ::MF2} should be compared with  \eqref{EQ:evolution_mf} and indicates that $\bx(t)$ and $\bu(t)$ correspond to the quantities $\E[\bX{t}]$ and $\E[\bU(t)]$ of the original stochastic system. As the constraint \eqref{EQ::MF3} must be ensured by $\bx(t)$ and $\bu(t)$, this constraint \eqref{EQ::POL} must be satisfied for the expectations: $\E[\bU(t)]\in\mcl{U}(\E[\bX{t}])$.

The reward collected at time $t$ for this dynamical system is $\Rvec{0}\cdot \bx(t) + (\Rvec{1} - \Rvec{0})\cdot\bu(t)$. Let $\lambda$ be the dual multiplier of the constraint \eqref{EQ::MF2-inf} of an optimal solution of \eqref{EQ::VOPTDYN-Tinf}. We define a \emph{deterministic} finite-horizon optimal control problem as:
%mapping an uncountable state space to an uncountable action space, $\pi : \Dels \to \mcl{U}$ is now expressed by:
% \begin{align}\label{EQ::VPOLDYN}
%     \Vpol{\infty}{\mB} =& \lim_{\THor \to \infty}\frac{1}{\THor}\sum_{t = 0}^{\THor - 1} \left( \Rvec{0}\cdot \mB(t) + (\Rvec{1} - \Rvec{0})\cdot\bu(t)\right),\\
%     \text{Subject to} & \text{ the dynamics \ref{EQ:MeanField}.}
% \end{align}
% Concretely, we will denote the set of all feasible policies that satisfy \ref{EQ:MeanField} by $\pspacedyn$. The corresponding \emph{deterministic optimal control problem} can be stated as follows:
\begin{subequations}
    \label{EQ::VOPTDYN}
    \begin{align}
       \VoptInf{\tau}{\mB(0)} &= \max_{\bx, \bu} \sum_{t = 0}^{\tau - 1} \left( \Rvec{0}\cdot \mB(t) + (\Rvec{1} - \Rvec{0})\cdot\bu(t)\right) + \lambda \cdot\bx(\tau),\label{EQ:VOPTDYN1}\\
         &\text{Subject to:} \text{ $\bx$  and $\bu$ satisfy \eqref{EQ:MeanField} for all $t\in\{0,\tau-1\}$,}
    %\max_{\pi \in \pspacedyn}\Vpol{\infty}{\mB}
    \end{align}
\end{subequations} 
Before moving forward, the equation above deserves some remarks. First, for any finite $\tau$, the objective and the constraints \eqref{EQ:MeanField} for the optimization problem \eqref{EQ::VOPTDYN} are linear in the variables $(\bx(t),\bu(t))$. This means that this optimization problem is computationally easy to solve. \textbf{In what follows, we denote by $\LPpol{\tau}{\mB}$ the value of $\bu(0)$ of an optimal solution to \eqref{EQ::VOPTDYN}.}

Second, the definition of \eqref{EQ::VOPTDYN} imposes that the constraint $\|\bu\|_1 \leq \alpha \|\mB\|_1 = \alpha $ holds for each time $t$. This is in contrast to the way this constraint is typically treated in RMAB problems, in which case \eqref{EQ::POL} is replaced with the time-averaged constraint $\frac{1}{T}\sum_{t = 0}^{T - 1}\|\bu(t)\|_1 \leq \alpha$. The latter relaxation was introduced in \cite{Wh88} and is often referred to as Whittle's relaxation \citep{Avrachenkov2020WhittleIB,Avrachenkov2021}. This is the constraint that we use to write  \eqref{EQ::VOPTDYN-Tinf}.  \cite{GGY23} showed that for any finite $T$, the finite-horizon equivalent of \eqref{EQ::VOPTT} converges to \eqref{EQ::VOPTDYN} as $N$ goes to infinity. The purpose of this paper is to show that the solution of the finite $T-$horizon LP \eqref{EQ::VOPTDYN} provides an almost-optimal solution to the original $N-$arm average reward problem \eqref{EQ::VOPTT}. 

Last, as we will discuss later, taking $\lambda$ as the dual multiplier of the constraint \eqref{EQ::MF2-inf} helps to make a connection between the finite and the infinite-horizon problems \eqref{EQ::VOPTDYN} and \eqref{EQ::VOPTDYN-Tinf}. Our proofs will hold with minor modification by replacing $\lambda$ by $0$ and in practice we do not use this multiplier at all.

\subsection{The Model Predictive Control Algorithm}
\begin{algorithm}[ht]
	\caption{Evaluation of the LP-Update policy}
	\label{algo::MPC}
	\begin{algorithmic}
            \REQUIRE Horizon $\tau$, Initial state $\bS^{(N)}(0)$, model parameters $\langle \Pmat{0}, \Pmat{1}, \Rvec{0}, \Rvec{1} \rangle$, and time horizon $T$
            \STATE Total-reward $\leftarrow 0$.
            \FOR{$t=0$ to $T-1$}
            \STATE $\bu(t) \leftarrow \LPpol{\tau}{\mB(\bS^{(N)}(t))}$.
            \STATE $\bA^{(N)}(t)$ $\leftarrow$ Randomized Rounding $(\bu(t))$ %(relegated to supplementary material)
            (by using Algorithm~\ref{algo::rounding}).
            \STATE Total-reward $\leftarrow$ Total-reward + $R(\bS^{(N)}(t), \bA^{(N)}(t))$.
            \STATE Simulate the transitions according to \eqref{EQ:MKEVOL} to get $\bS^{(N)}(t + 1)$            \ENDFOR
            \ENSURE{$\text{Average reward}: \frac{\text{Total-reward}}{\THor} \approx \Vlp{\tau}{N}$}
	\end{algorithmic}
\end{algorithm}

The pseudo-code of the LP-update policy is presented in Algorithm \ref{algo::MPC}. The LP-update policy takes as an input a time-horizon $\tau$. At each time-slot, the policy solves the finite horizon linear program \eqref{EQ::VOPTDYN} to obtain $\LPpol{\tau}{\mB}$ that is the value of $\bu(0)$ for an optimal solution to \eqref{EQ::VOPTDYN}. Note that such a policy may not immediately translate to an applicable policy as we do not require that $N\LPpol{\tau}{\mB}$ be integers. We therefore use \emph{randomized rounding} to obtain a feasible policy for our $N$ armed problem, $\bA^{N}(t)$. Applying these actions to each arm gives an instantaneous reward and a next state. This form of control has been referred to as \emph{rolling horizon} \citet{puterman2014markov} but is more commonly referred to as \emph{model predictive control} \citet{DTGLS14}. Our algorithm may be summarized by:
\begin{equation}\label{EQ::ALGO}
    \bS^{(N)}(t) \xrightarrow[]{\text{Solve LP \eqref{EQ::VOPTDYN}}}\LPpol{\tau}{\mB(\bS^{(N)}(t))} \xrightarrow[\text{rounding}]{\text{Randomized}} \bA^{N}(t) \xrightarrow[\text{new state}]{\text{Observe}} \bS^{(N)}(t + 1). 
\end{equation}
We use a randomized rounding procedure similar to \cite{GGY23}, %check the supplementary material for details.
see Appendix~\ref{apx:rounding}. 

