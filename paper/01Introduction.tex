\section{Introduction}
This work investigates the sequential decision making problem of Restless Multi-Armed Bandits (RMAB for short) over an infinite discrete time horizon. In this problem there are $N$ statistically identical arms. At each time step, the decision maker must choose for each arm if they would like to pull the arm or leave it as is. The decision maker has a constraint $\alpha N$ on the maximal number of arms that they may pull at each time instance. Each arm has a known state belonging to a common finite state space and upon choosing an action, produces a known state and action dependent reward. Next, the arms evolve to a new state independently according to a known state-action dependent transition kernel. These arms are only coupled through the budget constraint on the number of arms that may be pulled at each time instance. The state and reward are both revealed to the decision maker before the next decision needs to be made. The objective of the decision maker is to maximize the long-term time average reward.

This problem was first proposed by \citet{Wh88}. Over the years RMABs have been used to model a number of practical problems. These applications include web-crawling, queuing, communication systems, scheduling problems and many more, \citep{veatch1996scheduling}, \citep{dance2019optimal}, \citep{nino2002dynamic}, \citep{MearaWebcrawl01}. The problem of choosing a subset of tasks to perform among a larger collection of tasks under resource constraints shows up time and time again in various resource constrained control problems. For a comprehensive review on RMABs and their applications, the interested reader is directed towards \citet{NinoMora23}. While the existence of an optimal policy for RMABs is straightforward, \citet{PT99} showed that the exact solution to this problem is PSPACE-hard. Consequently, most work focused on designing approximate solutions with good performance guarantees. 

In the seminal paper, \citet{Wh88} suggested that under a condition known as indexability, an index can be associated with each state. This index is now referred to as the Whittle's Index (WI), and it was conjectured that a priority policy based on this index would be an optimal solution for this problem.  This setting naturally lends itself to mean field approximations where one may replace the $N$ armed problem with a dynamical system in order to find these approximate solutions. \citep{WW90} were the first to point out that under a (hard to verify) condition on the dynamical system known as uniform global attractor property (UGAP), the (WI) was asymptotically optimal. Recently, many of the results in the RMAB literature have focused on two major aspects of the problem: how quickly do proposed asymptotically optimal policies converge to the optimal solution as a function of the number of arms and whether the underlying assumptions such as indexability and UGAP can be made less restrictive to obtain more general conditions under which optimal solutions can be found. In the former category, under the assumptions of \citet{WW90}, \citet{GGY23} showed that the WI policy is exponentially close to the optimal solution. Recently, several works have been able to show an exponential order of convergence, \citet{GGY23b, HXCW24} without an indexability condition. On the other hand, in the latter category, \citet{HXCW23} showed an asymptotic convergence result under a less restrictive assumption known as the \emph{synchronization assumption}. Similar works include \citet{yan2024} and \citet{hong2024unichain} which show asymptotically optimal algorithms. Furthermore, \citet{HXCW24} loosen the restrictions to show asymptotic convergence and describe fundamental conditions to achieve exponential convergence rates for any algorithm. Our paper falls partially in the latter category by providing asymptotic convergence results under \emph{the weakest assumptions (to the best of our knowledge)} but we also show that under certain local stability assumptions one retrieves exponential convergence rates. A more comprehensive view of recent results can be found in the supplementary material.


\paragraph{Main Contributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The main contribution of our paper is the proof that a very natural model predictive control (LP-update) provides the \emph{best of both worlds}, it not only provides an algorithm that works well in practice, but we can also theoretically guarantee \emph{under the weakest assumptions (to the best of our knowledge) convergence to the optimal policy}. This idea of resolving an LP for a finite-horizon has been used previously; however, its use has been entirely restricted to either the finite-horizon case or, when used in the infinite horizon case, required a critical UGAP assumption(\emph{uniform global attractor property}) on the system. This assumption is known to be extremely hard to verify \cite{GGY23}. In this paper, by introducing the framework of \emph{dissipativity} we are able to show that a finite horizon policy indeed provides an almost optimal policy for the infinite horizon problem. Note, in the absence of such a framework, the finite horizon policy may veer very far from the infinite horizon policy \cite{DTGLS14}. The use of this framework is one of the key technical novelties of our approach. 
Apart from the main result, we make several fresh observations about the nature of RMAB problems:
\begin{itemize}[nosep]
    \item Our proof is of independent interest since it uses a new framework known as \emph{dissipativity}. Dissipativity is a closely studied phenomenon in the model predictive literature and is used to study how a policy drives a system towards optimal fixed points \citet{DTGLS14}. 
    \item Returning to the dynamics around the fixed point, we can tighten the rate of convergence to $e^{-cN}$ under a local stability condition.
    
    \item Perhaps the most helpful portion of our results, for practical purposes: the MPC algorithm works well in practice and is easy to implement. It performs well both in terms of the number of arms $N$ as well as the computational time horizon $T$, beating state-of-the-art algorithms in our bench marks. 
\redtext{
    \item Finally, from a control systems point of view, we return from trying to steer a dynamical system towards a fixed point to trying to maximize a value function. Policies can often be discontinuous in states while value functions are often smoother under mild assumptions.
}
\end{itemize}

\paragraph{Road-map} The rest of the paper is organized as follows. We describe the system model and the corresponding linear relaxation in Section~\ref{sec:model}. We build the LP-update algorithm in Section~\ref{sec:algo} and present its performance guarantee in Section~\ref{sec:main-theory}.  We provide the main ingredients of the proof in Section~\ref{sec:proof} postponing the most technical lemmas to the appendix.  We illustrate the performance of the algorithm in Section~\ref{sec:numerical}. The appendix contains additional literature review \ref{apx:review}, details of the algorithm and their extension to multi-constraints MDPs \ref{apx:algo}, additional proofs \ref{apx:PFcomp}, \ref{apx:PFcomp_expo} and details about the numerical experiments \ref{apx:parameters}.



